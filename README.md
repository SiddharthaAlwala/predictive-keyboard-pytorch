# Predictive Keyboard using PyTorch

A PyTorch-based **next-word prediction (predictive keyboard)** model built from scratch.  
The system learns language patterns from text and suggests the **top-3 next words**, similar to a smartphone keyboard.

---

## ğŸ“Œ What This Project Does

Given a partial sentence like:

i am going to

The model predicts likely next words such as:

be, do, get

This is implemented using a **word-level language model** trained on the Sherlock Holmes corpus.

---

## ğŸ§  Architecture Overview

- **Tokenization**: word-level (with punctuation handling)
- **Vocabulary**: frequency-based with `<pad>` and `<unk>`
- **Model**: Embedding â†’ LSTM â†’ Linear(vocab_size)

- **Loss**: CrossEntropyLoss
- **Optimizer**: Adam
- **Inference**: Top-K sampling with filters (keyboard-style)

---

## ğŸ“ Project Structure

    ```text
    predictive-keyboard-pytorch/
    â”œâ”€ README.md
    â”œâ”€ pyproject.toml                  # or requirements.txt (pick one)
    â”œâ”€ .gitignore
    â”œâ”€ .env.example                    # env vars like WANDB_API_KEY (optional)
    â”œâ”€ configs/
    â”‚  â”œâ”€ default.yaml                 # hyperparams, paths, model sizes
    â”‚  â””â”€ local.yaml                   # ignored; your machine-specific overrides
    â”œâ”€ data/
    â”‚  â”œâ”€ raw/
    â”‚  â”‚  â””â”€ sherlock_holmes.txt        # copy your uploaded dataset here
    â”‚  â”œâ”€ interim/                     # cleaned text, tokenized files (optional)
    â”‚  â””â”€ processed/
    â”‚     â”œâ”€ vocab.json                # stoi/itos, special tokens
    â”‚     â”œâ”€ train.pt                  # tensors / indexed sequences
    â”‚     â””â”€ valid.pt
    â”œâ”€ notebooks/
    â”‚  â””â”€ 01_explore_data.ipynb         # optional exploration
    â”œâ”€ src/
    â”‚  â””â”€ pkb/                         # "predictive keyboard" package
    â”‚     â”œâ”€ __init__.py
    â”‚     â”œâ”€ utils/
    â”‚     â”‚  â”œâ”€ seed.py
    â”‚     â”‚  â”œâ”€ logging.py
    â”‚     â”‚  â””â”€ io.py                   # load/save json, torch, text
    â”‚     â”œâ”€ data/
    â”‚     â”‚  â”œâ”€ preprocess.py           # clean + tokenize
    â”‚     â”‚  â”œâ”€ vocab.py                # build vocab + numericalize
    â”‚     â”‚  â”œâ”€ dataset.py              # PyTorch Dataset/DataLoader
    â”‚     â”‚  â””â”€ collate.py              # padding + batching
    â”‚     â”œâ”€ models/
    â”‚     â”‚  â”œâ”€ lstm_lm.py              # Embedding + LSTM + Linear
    â”‚     â”‚  â””â”€ sampling.py             # top-k, temperature, filters
    â”‚     â”œâ”€ train/
    â”‚     â”‚  â”œâ”€ train.py                # training loop
    â”‚     â”‚  â”œâ”€ eval.py                 # perplexity/accuracy
    â”‚     â”‚  â””â”€ checkpoints.py          # save/load checkpoints
    â”‚     â””â”€ inference/
    â”‚        â””â”€ predict.py              # given context -> top-3 suggestions
    â”œâ”€ scripts/
    â”‚  â”œâ”€ prepare_data.py               # raw -> processed
    â”‚  â”œâ”€ train.py                      # calls src/pkb/train/train.py
    â”‚  â””â”€ predict.py                    # CLI for suggestions
    â”œâ”€ tests/
    â”‚  â”œâ”€ test_vocab.py
    â”‚  â”œâ”€ test_dataset.py
    â”‚  â””â”€ test_sampling.py
    â”œâ”€ artifacts/
    â”‚  â”œâ”€ checkpoints/                  # model.pt, optimizer.pt
    â”‚  â””â”€ runs/                         # logs, metrics
    â””â”€ docs/
    â””â”€ design.md                     # notes: choices, experiments
    ```


## ğŸš€ How to Run

### 1ï¸âƒ£ Install dependencies
```bash
pip install torch

2ï¸âƒ£ Train the model
 python scripts/prepare_data.py 

This will:

build the vocabulary

train the LSTM language model

This will:
- build the vocabulary
- train the LSTM language model
- Save: 
    data/processed/vocab.json
    artifacts/checkpoints/best_model.pt

3ï¸âƒ£ Run the predictive keyboard
- python scripts/predict.py

Example:
    - Type something: i want to
    - Suggestions: ['go', 'get', 'see']

  - Type quit to exit.



 